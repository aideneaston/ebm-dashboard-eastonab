Data Source Reliability
Primary Data Sources

Data Source 1: HRIS (Workday)
- Reliability Rating: High – Demographic, employment, and turnover data are ~97% complete and reconciled with payroll.
- Collection & Updates: Employee self-reported demographics; hires, terminations, and job changes entered by HR; pay tied to payroll integration. Employment actions update in real time; demographics are refreshed quarterly with an annual verification.
- Key Limitations:
    - 3% of employees do not self-identify demographics, creating an “unknown” bucket.
    - Self-report and infrequent updates introduce some bias.

Data Source 2: LMS (Cornerstone)
- Reliability Rating: Medium–High – Training completion and hours are tracked well; behavior change is not.
- Collection & Updates: Automated tracking of enrollments, completions, scores, and hours; manager certifications via e-signature; 360 feedback for a subset of managers via CultureAmp. Data updates in real time; 360s are annual.
- Key Limitations:
    - Captures completion, not competence; high completion rates don't automatically translate to behavior change.
    - 360 coverage is only 42% and skewed toward higher-performing or volunteer managers → selection bias.

Measurement Quality
Validity
- Face Validity: High. The metrics clearly map onto the diversity (X) → inclusion/training (M) → performance (Y) logic. Demographics and representation capture X; training and inclusive leadership scores capture M (with some gaps); and turnover, engagement, promotion rates, and revenue per employee capture Y.
- Construct Validity: Medium–High.
    - Diversity (X) is measured well on race/ethnicity and gender, but other aspects of diversity are missing.
    - Inclusion mechanisms (M) are the weakest link: training completion is a rough proxy for real behavioral change; 360 data helps but has limited coverage and is perception-based.
    - Performance outcomes (Y) are generally strong (turnover, engagement, revenue, promotions) but don't cover all dimensions such as innovation quality or customer value creation in depth.
    - Multiple confounders (industry, tenure, team size, location, function, etc.) have been identified and partially controlled for, but not perfectly.
- Criterion Validity: High.
    - Organizational metrics line up well with external benchmarks (BLS, Gallup, Census, SHRM, etc.).
    - Internal relationships behave as theory predicts:
        - More training → lower diverse employee turnover.
        - Higher inclusive behavior scores → higher team engagement.

Reliability
- Objective Administrative Data: High reliability. HRIS, payroll, and LMS data are stable on re-query and subject to regular audits. Estimated error rates are below 2% for core fields.
- Subjective Data (surveys, 360s, ratings): Moderate reliability. Engagement and 360 scores show more variance over time, and inter-rater reliability for inclusive behaviors and performance ratings is around 0.58, which is below ideal. Still, internal consistency for survey constructs (Cronbach's alpha) is strong.
- Error Sources:
    - Systematic: bias in demographic self-reporting, rating bias (leniency/severity, recency, and demographic bias), response bias in surveys, and small distortions in training hours or revenue allocation.
    - Random: occasional data entry errors, inattentive survey responses, timing glitches in system integrations.
    - Human: misclassification of termination reasons, delays in data entry, and some calculation mistakes in earlier analyses (subsequently corrected).

Bias Assessment

Selection Bias
- Risk Level: Medium.
- Key Issues:
    - 360 feedback is skewed to a subset of managers, often higher performers.
    - Exit interview participation is lower among diverse leavers.
    - Small teams and some special populations are excluded from certain breakdowns for privacy reasons.
- Likely Effect: The organization probably has a more optimistic picture of inclusion than reality, especially among the managers and employees most at risk.

Survivorship Bias
- Risk Level: Medium–High.
- Key Issues:
    - Many earlier leavers (especially those who left years ago or very quickly) are not fully captured.
    - Senior diverse employees who "made it" may be unusually resilient or assimilated and not representative of the broader group.
- Likely Effect: The true historical severity of the diversity-without-inclusion problem is probably understated.

Measurement & Reporting Bias
- Gaming Risk: Low–Medium. Strong controls exist (audit trails, external audits, vendor-run surveys), but there is always some incentive to present better-looking results given executive and board scrutiny.
- Reporting Bias: Medium. Historically, success metrics (e.g., hiring diversity) were highlighted more than negative metrics (retention gaps) until more recent pressure for transparency. This has improved, but some selective framing likely remains.

Completeness Assessment

Coverage Over Time and Population
- Time Period:
    - Strong coverage for 2020–2022 across HRIS, LMS, engagement surveys, and financials.
    - Reasonable HR coverage from 2018–2022; pre-2018 is weaker.
    - This is adequate to understand the current problem and recent trends, but not to build a long historical narrative.
- Organizational Coverage:
    - About 95% of employees in scope; most major business units and locations are covered.
    - Contractors, international operations, and very small groups are underrepresented.
- Variable Coverage:
    - Core diversity and outcome metrics are well covered.
    - Mechanisms like psychological safety, sponsorship, informal networks, and microaggressions are only partially or indirectly measured.

Accuracy Assessment
- Verification:
    - HRIS headcount reconciles with payroll and EEO-1 reports.
    - Turnover, revenue, and training data have been cross-checked against independent systems and external benchmarks.
- Error Rates (Estimated):
    - Objective HR and financial data: <2% error.
    - Semi-subjective metrics (360s, ratings): 5–10%.

Organizational Context Assessment
Data Collection Environment & Culture
- Transparency & Politics: Leadership and the board have pushed for more honest visibility on diversity and inclusion–related outcomes. However, pockets of fear and defensiveness remain, particularly at the manager level, which can dampen full candor in surveys, exit interviews, and internal reporting.
- Technology & Process Constraints:
    - Systems are modern but still somewhat siloed, so a lot of integration happens manually in tools like Excel/Tableau.
    - Data entry and process delays (e.g., late terminations, infrequent demographic updates) affect timeliness.

Context-Specific Reliability

Problem-Specific Assessment
- Problem Detection: Very strong. Across multiple data sources, it is clear that the organization has successfully increased diversity in the workforce but has not matched that with inclusion and equitable outcomes. Diverse employees have higher turnover, slower promotions, and lower engagement than majority employees, and these gaps are sizable and consistent.
- Problem Severity: Well-estimated, within a reasonable range. The annual estimated cost of the problem (~$20.3M, with a ±15% range) is directionally reliable even if the exact dollar figure is not perfect.

Quality Ratings by Category

Performance Metrics
- Quality: Medium–High (7/10)
- Strengths: Strong, objective measures (turnover, promotion, tenure), robust sample sizes, and multiple indicators all point in the same direction.
- Limitations: Bias in ratings and perception data; innovation is only measured via proxies; survivorship bias is an ongoing concern.
- Usefulness: High for detecting and sizing the problem and for prioritizing where to intervene.

Financial Data
- Quality: High (9/10)
- Strengths: Audited, reconciled, and historically consistent; provides a solid foundation for ROI and cost estimates.
- Limitations: Revenue allocation assumptions and opportunity cost estimates introduce some uncertainty.
- Usefulness: Critical for making a compelling business case to executives and the board.

Operational Data
- Quality: Medium (6/10)
- Strengths: Training completion and core HR operations are tracked reliably.
- Limitations: Does not capture whether training actually changed skills or behavior; informal and relational aspects of inclusion are largely missing.
- Usefulness: Good for tracking implementation of interventions, weaker for diagnosing nuanced inclusion dynamics.

Customer/Market Data
- Quality: Medium–Low (5/10)
- Strengths: Reasonable customer satisfaction and win-rate data; some evidence connecting diverse, inclusive teams to better outcomes in diverse client accounts.
- Limitations: Sparse customer demographic data, small samples for diversity-related analyses, and multiple confounds.
- Usefulness: Helpful for supporting the business case but not strong enough to stand alone.

Overall Organizational Evidence Assessment

Key Strengths
- Clear, Converging Evidence of a Diversity-without-Inclusion Problem: Multiple independent data sources consistently show that diverse employees face worse outcomes in retention, advancement, and engagement, with real financial consequences.
- Strong Support for the X → M → Y Model: Internal data supports the idea that diversity (X) only translates into performance (Y) when inclusive mechanisms (M) are present. Units that combine diversity with strong inclusive leadership training and behaviors perform better than diverse but low-inclusion units and even better than homogeneous teams.

Main Limitations
- Survivorship Bias: The current dataset underrepresents those who already left due to poor inclusion, likely understating both the human and financial cost.
- Limited Historical Window: Most of the high-quality, comparable data is recent (past 3–5 years), so the organization cannot fully reconstruct the long-term trajectory of inclusion.

Confidence Level
- Overall confidence in using this data to justify action: Medium–High (~7.5/10).